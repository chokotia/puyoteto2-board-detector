# conv_pth2onnx.py
# ã‚«ã‚¹ã‚¿ãƒ MobileNet v3ï¼ˆãƒ†ãƒˆãƒªã‚¹ç›¤é¢åˆ†é¡ï¼‰ã‚’PyTorchã‹ã‚‰ONNXå½¢å¼ã«å¤‰æ›ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

import torch
from torch import nn
from torchvision import models

# --- ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®å®šç¾©ï¼ˆpredict_board.pyã¨åŒã˜ï¼‰ ---
def create_model():
    """ã‚«ã‚¹ã‚¿ãƒ MobileNet v3 smallãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ"""
    model = models.mobilenet_v3_small(weights=None)
    model.classifier = nn.Sequential(
        nn.Linear(model.classifier[0].in_features, 512),
        nn.Hardswish(),
        nn.Dropout(0.2),
        nn.Linear(512, 9)
    )
    return model

# --- è¨­å®š ---
model_path = "models/2025-08-10-0127/epoch_6_acc_100.00.pth"  # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
output_path = "tetris_mobilenet_v3_small_2025-08-10-0127.onnx"  # å‡ºåŠ›ONNXãƒ•ã‚¡ã‚¤ãƒ«å
# Webã‚¢ãƒ—ãƒªç”¨ã«ã¯CPUå›ºå®šãŒæ¨å¥¨ï¼ˆONNX.jsã¨ã®äº’æ›æ€§å‘ä¸Šï¼‰
device = torch.device("cpu")

# --- ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ãƒ»èª­ã¿è¾¼ã¿ ---
print("ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
model = create_model()
model.load_state_dict(torch.load(model_path, map_location=device))
model = model.to(device)
model.eval()
print(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {model_path}")

# --- ONNXå¤‰æ› ---
print("ONNXå½¢å¼ã«å¤‰æ›ä¸­...")
with torch.no_grad():
    # ãƒ€ãƒŸãƒ¼å…¥åŠ›ï¼ˆMobileNet v2ã®æ¨™æº–å…¥åŠ›ã‚µã‚¤ã‚ºï¼‰
    dummy_input = torch.randn(1, 3, 224, 224).to(device)
    
    # ONNXå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆONNX.jsäº’æ›æ€§é‡è¦–ï¼‰
    torch.onnx.export(
        model,                          # ãƒ¢ãƒ‡ãƒ«
        dummy_input,                    # ãƒ€ãƒŸãƒ¼å…¥åŠ›
        output_path,                    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
        export_params=True,             # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚‚å«ã‚ã‚‹
        opset_version=14,               # v3_smallã¯9ã§ã¯å‹•ä½œã—ãªã‹ã£ãŸãŸã‚14ã«ä¸Šã’ã‚‹
        do_constant_folding=True,       # å®šæ•°ç•³ã¿è¾¼ã¿æœ€é©åŒ–
        input_names=['input'],          # å…¥åŠ›å
        output_names=['output'],        # å‡ºåŠ›å
        dynamic_axes={
            'input': {0: 'batch_size'},
            'output': {0: 'batch_size'}
        },
        # ONNX.jsäº’æ›æ€§å‘ä¸Šã®ãŸã‚ã®è¿½åŠ ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        verbose=False,
        training=torch.onnx.TrainingMode.EVAL
    )

print(f"å¤‰æ›å®Œäº†: {output_path}")

# --- ä½¿ç”¨ä¾‹ ---
if __name__ == "__main__":
    import os
        
    print(f"\nâœ… å¤‰æ›å®Œäº†ï¼")
    print(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_path}")
    print(f"ğŸŒ Webã‚¢ãƒ—ãƒªã§ä½¿ç”¨å¯èƒ½ã§ã™")
    
    # ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã®èª¬æ˜
    print(f"\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æƒ…å ±:")
    print(f"   - å…¥åŠ›ã‚µã‚¤ã‚º: 224x224 RGB")
    print(f"   - å‡ºåŠ›ã‚¯ãƒ©ã‚¹æ•°: 9")
    print(f"   - ã‚¯ãƒ©ã‚¹: ãƒ†ãƒˆãƒªã‚¹ã®ã‚»ãƒ«çŠ¶æ…‹ (0-8)")
    print(f"   - å‰å‡¦ç†: Resize + ToTensor ã®ã¿ï¼ˆæ­£è¦åŒ–ãªã—ï¼‰")
