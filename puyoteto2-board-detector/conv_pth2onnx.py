# conv_pth2onnx.py
# ã‚«ã‚¹ã‚¿ãƒ MobileNet v2ï¼ˆãƒ†ãƒˆãƒªã‚¹ç›¤é¢åˆ†é¡ï¼‰ã‚’PyTorchã‹ã‚‰ONNXå½¢å¼ã«å¤‰æ›ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

import torch
from torch import nn
from torchvision import models, transforms
from PIL import Image
import numpy as np

# --- ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®å®šç¾©ï¼ˆpredict_board.pyã¨åŒã˜ï¼‰ ---
def create_model():
    """ã‚«ã‚¹ã‚¿ãƒ MobileNet v3 smallãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ"""
    model = models.mobilenet_v3_small(weights=None)
    model.classifier = nn.Sequential(
        nn.Linear(model.classifier[0].in_features, 512),
        nn.Hardswish(),
        nn.Dropout(0.2),
        nn.Linear(512, 9)
    )
    return model

# --- è¨­å®š ---
model_path = "models/2025-07-31-1942/epoch_7_acc_99.88.pth"  # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
output_path = "tetris_mobilenet_v3_small.onnx"  # å‡ºåŠ›ONNXãƒ•ã‚¡ã‚¤ãƒ«å
# Webã‚¢ãƒ—ãƒªç”¨ã«ã¯CPUå›ºå®šãŒæ¨å¥¨ï¼ˆONNX.jsã¨ã®äº’æ›æ€§å‘ä¸Šï¼‰
device = torch.device("cpu")

# --- ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ãƒ»èª­ã¿è¾¼ã¿ ---
print("ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
model = create_model()
model.load_state_dict(torch.load(model_path, map_location=device))
model = model.to(device)
model.eval()
print(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {model_path}")

# --- ONNXå¤‰æ› ---
print("ONNXå½¢å¼ã«å¤‰æ›ä¸­...")
with torch.no_grad():
    # ãƒ€ãƒŸãƒ¼å…¥åŠ›ï¼ˆMobileNet v2ã®æ¨™æº–å…¥åŠ›ã‚µã‚¤ã‚ºï¼‰
    dummy_input = torch.randn(1, 3, 224, 224).to(device)
    
    # ONNXå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆONNX.jsäº’æ›æ€§é‡è¦–ï¼‰
    torch.onnx.export(
        model,                          # ãƒ¢ãƒ‡ãƒ«
        dummy_input,                    # ãƒ€ãƒŸãƒ¼å…¥åŠ›
        output_path,                    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
        export_params=True,             # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚‚å«ã‚ã‚‹
        opset_version=14,               # v3_smallã¯9ã§ã¯å‹•ä½œã—ãªã‹ã£ãŸãŸã‚14ã«ä¸Šã’ã‚‹
        do_constant_folding=True,       # å®šæ•°ç•³ã¿è¾¼ã¿æœ€é©åŒ–
        input_names=['input'],          # å…¥åŠ›å
        output_names=['output'],        # å‡ºåŠ›å
        dynamic_axes={
            'input': {0: 'batch_size'},
            'output': {0: 'batch_size'}
        },
        # ONNX.jsäº’æ›æ€§å‘ä¸Šã®ãŸã‚ã®è¿½åŠ ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        verbose=False,
        training=torch.onnx.TrainingMode.EVAL
    )

print(f"å¤‰æ›å®Œäº†: {output_path}")

# --- ãƒ†ã‚¹ãƒˆç”¨ã®å‰å‡¦ç†é–¢æ•°ï¼ˆpredict_board.pyã¨åŒã˜ï¼‰ ---
def get_test_transform():
    """ãƒ†ã‚¹ãƒˆç”¨ã®å‰å‡¦ç†ï¼ˆå…ƒã®transform.pyã¨åŒã˜ï¼‰"""
    return transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor()
    ])

def preprocess_image_for_web(image_path):
    """Webç”¨ã®ç”»åƒå‰å‡¦ç†ï¼ˆæ­£è¦åŒ–ãªã—ç‰ˆï¼‰"""
    transform = get_test_transform()
    image = Image.open(image_path).convert('RGB')
    return transform(image).unsqueeze(0).numpy()

# --- PyTorchãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ ---
def test_pytorch_model(image_path):
    """PyTorchãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆæ¨è«–"""
    print(f"\nPyTorchãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ: {image_path}")
    
    if not os.path.exists(image_path):
        print(f"ãƒ†ã‚¹ãƒˆç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {image_path}")
        return
    
    transform = get_test_transform()
    image = Image.open(image_path).convert('RGB')
    input_tensor = transform(image).unsqueeze(0).to(device)
    
    with torch.no_grad():
        output = model(input_tensor)
        probabilities = torch.nn.functional.softmax(output, dim=1)
        predicted_class = torch.argmax(output, dim=1).item()
        confidence = probabilities[0][predicted_class].item()
        
    print(f"äºˆæ¸¬ã‚¯ãƒ©ã‚¹: {predicted_class}")
    print(f"ä¿¡é ¼åº¦: {confidence:.3f}")
    print(f"å…¨ã‚¯ãƒ©ã‚¹ç¢ºç‡: {probabilities[0].tolist()}")

# --- ä½¿ç”¨ä¾‹ ---
if __name__ == "__main__":
    import os
    
    # ãƒ†ã‚¹ãƒˆç”»åƒã§ã®ãƒ†ã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    test_image_path = "test_cell.png"  # ãƒ†ã‚¹ãƒˆç”¨ã®ã‚»ãƒ«ç”»åƒãŒã‚ã‚Œã°
    if os.path.exists(test_image_path):
        test_pytorch_model(test_image_path)
    else:
        print(f"\nãƒ†ã‚¹ãƒˆç”»åƒ {test_image_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("ãƒ†ã‚¹ãƒˆç”»åƒãŒã‚ã‚‹å ´åˆã¯ã€ãƒ‘ã‚¹ã‚’ä¿®æ­£ã—ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„")
    
    print(f"\nâœ… å¤‰æ›å®Œäº†ï¼")
    print(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_path}")
    print(f"ğŸŒ Webã‚¢ãƒ—ãƒªã§ä½¿ç”¨å¯èƒ½ã§ã™")
    
    # ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã®èª¬æ˜
    print(f"\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æƒ…å ±:")
    print(f"   - å…¥åŠ›ã‚µã‚¤ã‚º: 224x224 RGB")
    print(f"   - å‡ºåŠ›ã‚¯ãƒ©ã‚¹æ•°: 9")
    print(f"   - ã‚¯ãƒ©ã‚¹: ãƒ†ãƒˆãƒªã‚¹ã®ã‚»ãƒ«çŠ¶æ…‹ (0-8)")
    print(f"   - å‰å‡¦ç†: Resize + ToTensor ã®ã¿ï¼ˆæ­£è¦åŒ–ãªã—ï¼‰")
